{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Lab7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dMWir3WM0s0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7yt3LF749HK",
        "colab_type": "code",
        "outputId": "d602c092-3dae-4b2f-be0d-b22b2a359951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "cell_type": "code",
      "source": [
        "#Reading data\n",
        "data = input_data.read_data_sets('data/fashion',one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-c16302e8c919>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7wIyDaqA5dKp",
        "colab_type": "code",
        "outputId": "786e11e5-eb56-4a3f-dcc2-af01a8e466b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# Shapes of training set\n",
        "print(\"Training set (images) shape: {shape}\".format(shape=data.train.images.shape))\n",
        "print(\"Training set (labels) shape: {shape}\".format(shape=data.train.labels.shape))\n",
        "\n",
        "# Shapes of test set\n",
        "print(\"Test set (images) shape: {shape}\".format(shape=data.test.images.shape))\n",
        "print(\"Test set (labels) shape: {shape}\".format(shape=data.test.labels.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (images) shape: (55000, 784)\n",
            "Training set (labels) shape: (55000, 10)\n",
            "Test set (images) shape: (10000, 784)\n",
            "Test set (labels) shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AGJqvPP_5yLo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape training and testing image\n",
        "train_X = data.train.images.reshape(-1, 28, 28, 1)\n",
        "test_X = data.test.images.reshape(-1,28,28,1)\n",
        "train_y = data.train.labels\n",
        "test_y = data.test.labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xcsGM0gq70Pf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Our Model:\n",
        "<ol>\n",
        "<li>Convolution layer with 32-3x3 filters</li>\n",
        "<li>Max pooling layer 2x2</li>\n",
        "<li>Convolution layer with 64-3x3 filters</li>\n",
        "<li>Max pooling layer 2x2</li>\n",
        "<li>Convolution layer with 128-3x3 filters</li>\n",
        "<li>Max pooling layer 2x2</li>\n",
        "<li>Fully-connected layer of 128 units</li>\n",
        "<li>Output layer of 10 units</li>\n",
        "</ol>\n",
        "\n",
        "Architecture drawn using: http://alexlenail.me/NN-SVG/AlexNet.html\n",
        "\n",
        "<img src=\"https://i.imgur.com/DGGlRcK.png\" alt=\"neural-network\" border=\"0\">"
      ]
    },
    {
      "metadata": {
        "id": "0aVeNxxW6PSp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iters = 15    \n",
        "learning_rate = 0.001   \n",
        "batch_size = 256        #Mini-batch size\n",
        "n_input = 28            #img_shape: 28x28\n",
        "n_classes = 10          #no_classes: 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GRrUnMRWDDZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define layers (wrappers) and initializer\n",
        "\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # implement the conv2d function to make a convolution layer and applies relu \n",
        "    # as activation function \n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')\n",
        "  \n",
        "  \n",
        "def initializer():  \n",
        "  weights = {\n",
        "      'wc1': tf.get_variable('W0', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "      # complete the missing weights for the second layer \n",
        "      'wc3': tf.get_variable('W2', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "      'wd1': tf.get_variable('W3', shape=(4*4*128,128), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "      'out': tf.get_variable('W6', shape=(128,n_classes), initializer=tf.contrib.layers.xavier_initializer()), \n",
        "  }\n",
        "  biases = {\n",
        "      'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "      'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "      'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "      'bd1': tf.get_variable('B3', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "      'out': tf.get_variable('B4', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),\n",
        "  }\n",
        "  return weights, biases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PCAPueCYDTfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Model\n",
        "\n",
        "def conv_net(x, weights, biases):  \n",
        "    \n",
        "    # Convolution layers\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
        "    conv3 = maxpool2d(conv3, k=2)\n",
        "\n",
        "\n",
        "    # Fully connected layer\n",
        "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    \n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VK9HHl1FEgtK",
        "colab_type": "code",
        "outputId": "aaa0079f-b861-44ef-f009-43c32b1cabfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.placeholder(\"float\", [None, 28,28,1])\n",
        "y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "weights, biases = initializer()\n",
        "# Applying the model to our data\n",
        "pred = conv_net(x, weights, biases)\n",
        "# Defining the cost function\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "# Defining the optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-8-b1a4b4ad60f1>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r_InB41cG3Hj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.\n",
        "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "#calculate accuracy across all the given images and average them out. \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cmUS26yVEzUo",
        "colab_type": "code",
        "outputId": "fbabff53-e659-438e-b9fe-aa590c05e86a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1097
        }
      },
      "cell_type": "code",
      "source": [
        "# Running session\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init) \n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "    train_accuracy = []\n",
        "    test_accuracy = []\n",
        "    #Write to graph\n",
        "    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n",
        "    for i in range(iters):\n",
        "        start = time.time()\n",
        "        for batch in range(len(train_X)//batch_size):\n",
        "            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
        "            batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n",
        "            # Calculate batch loss and accuracy\n",
        "            opt = sess.run(optimizer, feed_dict={x: batch_x,\n",
        "                                                              y: batch_y})\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
        "                                                              y: batch_y})\n",
        "        print(\"(\" + str(i+1) + \"/\" + str(iters) + \")\" \", Loss= \" + \\\n",
        "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                      \"{:.5f}\".format(acc))\n",
        "        end = time.time()\n",
        "\n",
        "        # Calculate accuracy for all test images\n",
        "        test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})\n",
        "        train_loss.append(loss)\n",
        "        test_loss.append(valid_loss)\n",
        "        train_accuracy.append(acc)\n",
        "        test_accuracy.append(test_acc)\n",
        "        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
        "        print(\"Time taken for one epoch: \" + str(end-start) + \" seconds\")\n",
        "        print(\"\")\n",
        "    summary_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1/15), Loss= 0.196493, Training Accuracy= 0.96094\n",
            "Testing Accuracy: 0.96470\n",
            "Time taken for one epoch: 7.713947296142578 seconds\n",
            "\n",
            "(2/15), Loss= 0.134302, Training Accuracy= 0.96875\n",
            "Testing Accuracy: 0.97810\n",
            "Time taken for one epoch: 5.12232780456543 seconds\n",
            "\n",
            "(3/15), Loss= 0.092068, Training Accuracy= 0.97656\n",
            "Testing Accuracy: 0.98400\n",
            "Time taken for one epoch: 5.097223997116089 seconds\n",
            "\n",
            "(4/15), Loss= 0.072345, Training Accuracy= 0.98047\n",
            "Testing Accuracy: 0.98720\n",
            "Time taken for one epoch: 4.797422409057617 seconds\n",
            "\n",
            "(5/15), Loss= 0.057555, Training Accuracy= 0.98047\n",
            "Testing Accuracy: 0.98840\n",
            "Time taken for one epoch: 4.846061706542969 seconds\n",
            "\n",
            "(6/15), Loss= 0.034990, Training Accuracy= 0.98047\n",
            "Testing Accuracy: 0.98890\n",
            "Time taken for one epoch: 4.920878648757935 seconds\n",
            "\n",
            "(7/15), Loss= 0.022111, Training Accuracy= 0.99219\n",
            "Testing Accuracy: 0.98750\n",
            "Time taken for one epoch: 4.893008470535278 seconds\n",
            "\n",
            "(8/15), Loss= 0.012125, Training Accuracy= 0.99609\n",
            "Testing Accuracy: 0.98720\n",
            "Time taken for one epoch: 4.924456357955933 seconds\n",
            "\n",
            "(9/15), Loss= 0.007467, Training Accuracy= 1.00000\n",
            "Testing Accuracy: 0.98410\n",
            "Time taken for one epoch: 5.079885482788086 seconds\n",
            "\n",
            "(10/15), Loss= 0.007943, Training Accuracy= 1.00000\n",
            "Testing Accuracy: 0.97640\n",
            "Time taken for one epoch: 4.890422821044922 seconds\n",
            "\n",
            "(11/15), Loss= 0.008842, Training Accuracy= 0.99609\n",
            "Testing Accuracy: 0.98840\n",
            "Time taken for one epoch: 4.812413692474365 seconds\n",
            "\n",
            "(12/15), Loss= 0.012194, Training Accuracy= 0.99219\n",
            "Testing Accuracy: 0.98790\n",
            "Time taken for one epoch: 4.87730860710144 seconds\n",
            "\n",
            "(13/15), Loss= 0.005043, Training Accuracy= 1.00000\n",
            "Testing Accuracy: 0.98830\n",
            "Time taken for one epoch: 4.666110992431641 seconds\n",
            "\n",
            "(14/15), Loss= 0.002057, Training Accuracy= 1.00000\n",
            "Testing Accuracy: 0.98810\n",
            "Time taken for one epoch: 4.701019287109375 seconds\n",
            "\n",
            "(15/15), Loss= 0.004322, Training Accuracy= 1.00000\n",
            "Testing Accuracy: 0.98730\n",
            "Time taken for one epoch: 4.810177803039551 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wUtvO4FKQJ6U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Connecting to TPU: https://stackoverflow.com/questions/52530578/how-use-tpu-in-google-colab"
      ]
    },
    {
      "metadata": {
        "id": "1b6ntcu3N9cQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ResNet 50"
      ]
    },
    {
      "metadata": {
        "id": "4L6NjBBKRfyN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>Deep Networks Problem (Vanishing Gradients) </h3>\n",
        "<p> because of multiplication during backpropagation</p>\n",
        "</br>\n",
        "<img src=\"https://github.com/priya-dwivedi/Deep-Learning/raw/582d6c154cb8b3a0ea39ab77012e9bed0c831473/resnet_keras/images/vanishing_grad_kiank.png\" alt=\"neural-network\" border=\"0\">\n",
        "\n",
        "Solution: ResNet blocks with the shortcut makes it very easy for one of the blocks to learn an identity function\n",
        "<h3>Difference between shortcut and no shortcut </h3>\n",
        "</br>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*-_ED04HNCNz7HFyqQtbTtg.png\" alt=\"neural-network\" border=\"0\">\n",
        "<h3>ResNet architecture</h3>\n",
        "</br>\n",
        "<img src=\"https://github.com/priya-dwivedi/Deep-Learning/raw/582d6c154cb8b3a0ea39ab77012e9bed0c831473/resnet_keras/images/resnet_kiank.png\" alt=\"neural-network\" border=\"0\">\n",
        "<h3>Difference between ResNet and ResNeXt </h3>\n",
        "</br>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*LOoc11tkDoqv0pC6OH7mwA.png\" alt=\"neural-network\" border=\"0\">"
      ]
    },
    {
      "metadata": {
        "id": "JQFMbNRFTbQd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "\n",
        "#\n",
        "# image dimensions\n",
        "#\n",
        "\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "img_channels = 3\n",
        "\n",
        "#\n",
        "# network params\n",
        "#\n",
        "\n",
        "cardinality = 1\n",
        "\n",
        "\n",
        "def residual_network(x):\n",
        "    \"\"\"\n",
        "    ResNeXt by default. For ResNet set `cardinality` = 1 above.\n",
        "    \n",
        "    \"\"\"\n",
        "    def add_common_layers(y):\n",
        "        y = layers.BatchNormalization()(y)\n",
        "        y = layers.LeakyReLU()(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def grouped_convolution(y, nb_channels, _strides):\n",
        "        # when `cardinality` == 1 this is just a standard convolution\n",
        "        if cardinality == 1:\n",
        "            return layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
        "        \n",
        "#         assert not nb_channels % cardinality\n",
        "#         _d = nb_channels // cardinality\n",
        "\n",
        "#         # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
        "#         # and convolutions are separately performed within each group\n",
        "#         groups = []\n",
        "#         for j in range(cardinality):\n",
        "#             group = layers.Lambda(lambda z: z[:, :, :, j * _d:j * _d + _d])(y)\n",
        "#             groups.append(layers.Conv2D(_d, kernel_size=(3, 3), strides=_strides, padding='same')(group))\n",
        "            \n",
        "#         # the grouped convolutional layer concatenates them as the outputs of the layer\n",
        "#         y = layers.concatenate(groups)\n",
        "\n",
        "#         return y\n",
        "\n",
        "    def residual_block(y, nb_channels_in, nb_channels_out, _strides=(1, 1), _project_shortcut=False):\n",
        "        \"\"\"\n",
        "        Our network consists of a stack of residual blocks. These blocks have the same topology,\n",
        "        and are subject to two simple rules:\n",
        "        - If producing spatial maps of the same size, the blocks share the same hyper-parameters (width and filter sizes).\n",
        "        - Each time the spatial map is down-sampled by a factor of 2, the width of the blocks is multiplied by a factor of 2.\n",
        "        \"\"\"\n",
        "        shortcut = y\n",
        "\n",
        "        # we modify the residual building block as a bottleneck design to make the network more economical\n",
        "        y = layers.Conv2D(nb_channels_in, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
        "        y = add_common_layers(y)\n",
        "\n",
        "        # ResNeXt (identical to ResNet when `cardinality` == 1)\n",
        "        y = grouped_convolution(y, nb_channels_in, _strides=_strides)\n",
        "        y = add_common_layers(y)\n",
        "\n",
        "        y = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=(1, 1), padding='same')(y)\n",
        "        # batch normalization is employed after aggregating the transformations and before adding to the shortcut\n",
        "        y = layers.BatchNormalization()(y)\n",
        "\n",
        "        # identity shortcuts used directly when the input and output are of the same dimensions\n",
        "        if _project_shortcut or _strides != (1, 1):\n",
        "            # when the dimensions increase projection shortcut is used to match dimensions (done by 1Ã—1 convolutions)\n",
        "            # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
        "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
        "            shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "        y = layers.add([shortcut, y])\n",
        "\n",
        "        # relu is performed right after each batch normalization,\n",
        "        # expect for the output of the block where relu is performed after the adding to the shortcut\n",
        "        y = layers.LeakyReLU()(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    # conv1\n",
        "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(x)\n",
        "    x = add_common_layers(x)\n",
        "\n",
        "    # conv2\n",
        "    x = layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
        "    for i in range(3):\n",
        "        project_shortcut = True if i == 0 else False\n",
        "        x = residual_block(x, 128, 256, _project_shortcut=project_shortcut)\n",
        "\n",
        "    # conv3\n",
        "    for i in range(4):\n",
        "        # down-sampling is performed by conv3_1, conv4_1, and conv5_1 with a stride of 2\n",
        "        strides = (2, 2) if i == 0 else (1, 1)\n",
        "        x = residual_block(x, 256, 512, _strides=strides)\n",
        "\n",
        "    # conv4\n",
        "    for i in range(6):\n",
        "        strides = (2, 2) if i == 0 else (1, 1)\n",
        "        x = residual_block(x, 512, 1024, _strides=strides)\n",
        "\n",
        "    # conv5\n",
        "    for i in range(3):\n",
        "        strides = (2, 2) if i == 0 else (1, 1)\n",
        "        x = residual_block(x, 1024, 2048, _strides=strides)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(1)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "image_tensor = layers.Input(shape=(img_height, img_width, img_channels,))\n",
        "network_output = residual_network(image_tensor)\n",
        "  \n",
        "model = models.Model(inputs=[image_tensor], outputs=[network_output])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjrY9sIzTdxq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inception v3"
      ]
    },
    {
      "metadata": {
        "id": "FWnVx4WBXWzK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3>WHY USE 1 LETS USE ALL</h3>\n",
        "In a typical CNN layer, we make a choice to either have a 3x3 filters, or 5x5 filters or a max pooling layer. In general all of these are beneficial . The inception block use them all.\n",
        "<h3>Efficiency</h3>\n",
        "Use (1x1) Conv Layer to reduce dimensionality, then perform a regular conv layer, finally use another (1x1) to increase dimensionality (not used in inception).\n",
        "</br>\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*aq4tcBl9t5Z36kTDeZSOHA.png\" alt=\"neural-network\" border=\"0\">"
      ]
    },
    {
      "metadata": {
        "id": "VH3d8K0zTccF",
        "colab_type": "code",
        "outputId": "351631e0-43e1-4332-d7ce-863926014ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Flatten, Dense\n",
        "input_img = Input(shape = (32, 32, 3))\n",
        "\n",
        "tower_1 = Conv2D(64, (1,1), padding='same', activation='relu')(input_img)\n",
        "tower_1 = Conv2D(64, (3,3), padding='same', activation='relu')(tower_1)\n",
        "tower_2 = Conv2D(64, (1,1), padding='same', activation='relu')(input_img)\n",
        "tower_2 = Conv2D(64, (5,5), padding='same', activation='relu')(tower_2)\n",
        "tower_3 = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_img)\n",
        "tower_3 = Conv2D(64, (1,1), padding='same', activation='relu')(tower_3)\n",
        "\n",
        "output = concatenate([tower_1, tower_2, tower_3], axis = 3)\n",
        "\n",
        "output = Flatten()(output)\n",
        "out    = Dense(1, activation='sigmoid')(output)\n",
        "\n",
        "model = keras.Model(inputs = input_img, outputs = out)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1166 (Conv2D)            (None, 32, 32, 64)   256         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1168 (Conv2D)            (None, 32, 32, 64)   256         input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 3)    0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1167 (Conv2D)            (None, 32, 32, 64)   36928       conv2d_1166[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1169 (Conv2D)            (None, 32, 32, 64)   102464      conv2d_1168[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1170 (Conv2D)            (None, 32, 32, 64)   256         max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 32, 32, 192)  0           conv2d_1167[0][0]                \n",
            "                                                                 conv2d_1169[0][0]                \n",
            "                                                                 conv2d_1170[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 196608)       0           concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            196609      flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 336,769\n",
            "Trainable params: 336,769\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RFo6c4ZGYjb4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}